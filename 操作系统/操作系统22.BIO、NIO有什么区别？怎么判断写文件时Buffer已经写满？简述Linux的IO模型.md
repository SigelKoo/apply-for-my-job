# BIO、NIO有什么区别？怎么判断写文件时Buffer已经写满？简述Linux的IO模型

### BIO、NIO有什么区别？

#### IO的阻塞

- 用系统调用`read`从socket里读取一段数据
- 用系统调用`read`从一个磁盘文件读取一段数据到内存

如果你的直觉告诉你，这两种都算阻塞，那么很遗憾，你的理解与Linux不同。Linux认为：

- 对于第一种情况，算作block，因为Linux无法知道网络上对方是否会发数据。如果没数据发过来，对于调用`read`的程序来说，就只能“等”。
- 对于第二种情况，**不算做阻塞**。

>一个解释是，所谓阻塞是指操作系统可以预见这个阻塞会发生才会主动阻塞。例如当读取TCP连接的数据时，如果发现socket buffer里没有数据就可以确定定对方还没有发过来，于是阻塞；而对于普通磁盘文件的读写，也许磁盘运作期间会抖动，会短暂暂停，但是操作系统无法预见这种情况，只能视作不会阻塞，照样执行。

基于这个基本的设定，在讨论IO时，一定要严格区分网络IO和磁盘文件IO。NIO和IO多路复用只对网络IO有意义。

#### BIO

BIO是Blocking IO的意思。在类似于网络中进行`read`，`write`，`connect`一类的系统调用时会被卡住。

举个例子，当用`read`去读取网络的数据时，是无法预知对方是否已经发送数据的。因此在收到数据之前，能做的只有等待，直到对方把数据发过来，或者等到网络超时。等待期间不可处理第二个客户端建立的连接。

对于单线程的网络服务，这样做就会有卡死的问题。因为当等待时，整个线程会被挂起，无法执行，也无法做其他的工作。

> 顺便说一句，这种Block是不会影响同时运行的其他程序（进程）的，因为现代操作系统都是多任务的，任务之间的切换是抢占式的。这里Block只是指Block当前的进程。

于是，网络服务为了同时响应多个并发的网络请求，必须实现为多线程的。每个线程处理一个网络请求。线程数随着并发连接数线性增长。这的确能奏效。但这带来两个问题：

- 线程越多，Context Switch就越多，而Context Switch是一个比较重的操作，会无谓浪费大量的CPU。上下文切换（一个进程/线程切换到另一个进程/线程）。
- 每个线程会占用一定的内存作为线程的栈。比如有1000个线程同时运行，每个占用1MB内存，就占用了1个G的内存。

问题的关键在于，当调用`read`接受网络请求时，有数据到了就用，没数据到时，实际上是可以干别的。使用大量线程，仅仅是因为Block发生，没有其他办法。

当然你可能会说，是不是可以弄个线程池呢？这样既能并发的处理请求，又不会产生大量线程。但这样会限制最大并发的连接数。比如你弄4个线程，那么最大4个线程都Block了就没法响应更多请求了。

要是操作IO接口时，操作系统能够总是直接告诉有没有数据，而不是Block去等就好了。于是，NIO登场。

#### NIO

NIO是指将IO模式设为“Non-Blocking”模式，不让用户阻塞。使用单线程解决高并发的问题。

###### 区别

在BIO模式下，调用read，如果发现没数据已经到达，就会阻塞住。

在NIO模式下，调用read，如果发现没数据已经到达，就会立刻返回-1，并且errno被设为`EAGAIN`，提示你的应用程序现在没有数据可读请稍后再试。

不让程序阻塞

1. 等待连接和读数据不阻塞
2. 有人来连，设置为非阻塞读，将当前socket放入到list，遍历list，查看是否有人发数据
3. 没人来连，将当前socket放入到list，遍历list，查看是否有人发数据

每次循环的时候，都会遍历list列表，如果列表很大，若只有20%有读写，性能还是较低

使用轮询，不断的尝试有没有数据到达，有了就处理，没有（得到`EWOULDBLOCK`或者`EAGAIN`）就等一小会再试。这比之前BIO好多了，起码程序不会被卡死了。

但这样会带来两个新问题：

- 如果有大量文件描述符都要等，那么就得一个一个的read。这会带来大量的Context Switch（`read`是系统调用，每调用一次就得在用户态和核心态切换一次）。
- 休息一会的时间不好把握。这里是要猜多久之后数据才能到。等待时间设的太长，程序响应延迟就过大；设的太短，就会造成过于频繁的重试，干耗CPU而已。

要是操作系统能一口气告诉程序，哪些数据到了就好了。

#### IO多路复用

IO多路复用（IO Multiplexing) 是这么一种机制：程序注册一组socket文件描述符给操作系统，表示“我要监视这些fd是否有IO事件发生，有了就告诉程序处理”。

IO多路复用是要和NIO一起使用的。尽管在操作系统级别，NIO和IO多路复用是两个相对独立的事情。**NIO仅仅是指IO API总是能立刻返回，不会被Blocking**；而**IO多路复用仅仅是操作系统提供的一种便利的通知机制**。操作系统并不会强制这俩必须得一起用——你可以用NIO，但不用IO多路复用，就像上一节中的代码；也可以只用IO多路复用 + BIO，这时效果还是当前线程被卡住。但是，IO多路复用和NIO是要配合一起使用才有实际意义。因此，在使用IO多路复用之前，请总是先把fd设为`O_NONBLOCK`。

将list交给操作系统来处理，具体的方法有三个：select、poll和epoll。

#### 怎么判断写文件时Buffer已经写满？

1. 使用copy_from_user从用户空间拷贝fd_set到内核空间

2. 注册回调函数__pollwait

3. 遍历所有fd，调用其对应的poll方法（对于socket，这个poll方法是sock_poll，sock_poll根据情况会调用到tcp_poll,udp_poll或者datagram_poll）
4. 以tcp_poll为例，其核心实现就是__pollwait，也就是上面注册的回调函数。
5. __pollwait的主要工作就是把current（当前进程）挂到设备的等待队列中，不同的设备有不同的等待队列，对于tcp_poll来说，其等待队列是sk->sk_sleep（注意把进程挂到等待队列中并不代表进程已经睡眠了）。在设备收到一条消息（网络设备）或填写完文件数据（磁盘设备）后，会唤醒设备等待队列上睡眠的进程，这时current便被唤醒了。

6. poll方法返回时会返回一个描述读写操作是否就绪的mask掩码，根据这个mask掩码给fd_set赋值。

7. 如果遍历完所有的fd，还没有返回一个可读写的mask掩码，则会调用schedule_timeout是调用select的进程（也就是current）进入睡眠。当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程。如果超过一定的超时时间（schedule_timeout指定），还是没人唤醒，则调用select的进程会重新被唤醒获得CPU，进而重新遍历fd，判断有没有就绪的fd。

8. 把fd_set从内核空间拷贝到用户空间

# 简述Linux的IO模型

### Linux IO模型分类

- 阻塞 I/O
- 非阻塞 I/O
- I/O多路复用
- 信号驱动式I/O
- 异步I/O

**POSIX（可移植操作系统接口）把同步IO操作定义为导致进程阻塞直到IO完成的操作，反之则是异步IO**

这里统一使用Linux下的系统调用recv作为例子，它用于从套接字上接收一个消息，因为是一个系统调用，所以调用时会从用户进程空间切换到内核空间运行一段时间再切换回来。默认情况下recv会等到网络数据到达并且复制到用户进程空间或者发生错误时返回，而第4个参数flags可以让它马上返回。

##### 阻塞I/O

使用recv的默认参数一直等数据直到拷贝到用户空间，这段时间内进程始终阻塞。A同学用杯子装水，打开水龙头装满水然后离开。这一过程就可以看成是使用了阻塞IO模型，因为如果水龙头没有水，他也要等到有水并装满杯子才能离开去做别的事情。很显然，这种IO模型是同步的。

![image](http://image.euphie.net/2017-09-24-23-18-01.png)

##### 非阻塞 I/O

改变flags，让recv不管有没有获取到数据都返回，如果没有数据那么一段时间后再调用recv看看，如此循环。B同学也用杯子装水，打开水龙头后发现没有水，它离开了，过一会他又拿着杯子来看看……在中间离开的这些时间里，B同学离开了装水现场（回到用户进程空间），可以做他自己的事情。这就是非阻塞IO模型。但是它只有是检查无数据的时候是非阻塞的，在数据到达的时候依然要等待复制数据到用户空间（等着水将水杯装满），因此它还是同步IO。

![image](http://image.euphie.net/2017-09-24-23-19-53.png)

当一个应用进程循环调用recvfrom的时候，这种操作叫做轮询。应用进程轮询内核，检查某个操作是否准备就绪，当IO操作准备就绪可以操作的时候就会进行真正的IO操作，就是将数据从内核写入用户空间的过程。但是这样做会导致CPU的大量耗费。

##### I/O多路复用

这里在调用recv前先调用select或者poll，这2个系统调用都可以在内核准备好数据（网络数据到达内核）时告知用户进程，这个时候再调用recv一定是有数据的。因此这一过程中它是阻塞于select或poll，而没有阻塞于recv，有人将非阻塞IO定义成在读写操作时没有阻塞于系统调用的IO操作（不包括数据从内核复制到用户空间时的阻塞，因为这相对于网络IO来说确实很短暂），如果按这样理解，这种IO模型也能称之为非阻塞IO模型，但是按POSIX来看，它也是同步IO，那么也和楼上一样称之为同步非阻塞IO吧。

这种IO模型比较特别，分个段。因为它能同时监听多个文件描述符(fd)。这个时候C同学来装水，发现有一排水龙头，舍管阿姨告诉他这些水龙头都还没有水，等有水了告诉他。于是等啊等(select调用中)，过了一会阿姨告诉他有水了，但不知道是哪个水龙头有水，自己看吧。于是C同学一个个打开，往杯子里装水（recv）。这里再顺便说说鼎鼎大名的epoll(高性能的代名词啊)，epoll也属于IO复用模型，主要区别在于舍管阿姨会告诉C同学哪几个水龙头有水了，不需要一个个打开看(当然还有其它区别)。

![image](http://image.euphie.net/2017-09-24-23-21-54.png)

##### 信号驱动式I/O

通过调用sigaction注册信号函数，等内核数据准备好的时候系统中断当前程序，执行信号函数（在这里面调用recv）。D同学让舍管阿姨等有水的时候通知他（注册信号函数），没多久D同学得知有水了，跑去装水。是不是很像异步IO？很遗憾，它还是同步IO（省不了装水的时间啊）。

![image](http://image.euphie.net/2017-09-24-23-22-38.png)

##### 异步I/O

调用aio_read，让内核等数据准备好，并且复制到用户进程空间后执行事先指定好的函数。E同学让舍管阿姨将杯子装满水后通知他。整个过程E同学都可以做别的事情（没有recv），这才是真正的异步IO。

![image](http://image.euphie.net/2017-09-24-23-23-36.png)

### 区别

![img](https://images2015.cnblogs.com/blog/1066890/201611/1066890-20161129014959615-1351089676.png)