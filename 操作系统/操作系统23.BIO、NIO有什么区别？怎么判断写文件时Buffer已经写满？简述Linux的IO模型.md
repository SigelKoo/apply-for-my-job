# BIO、NIO有什么区别？怎么判断写文件时Buffer已经写满？简述Linux的IO模型

### BIO、NIO有什么区别？

#### IO的阻塞

- 用系统调用`read`从socket里读取一段数据
- 用系统调用`read`从一个磁盘文件读取一段数据到内存

如果你的直觉告诉你，这两种都算阻塞，那么很遗憾，你的理解与Linux不同。Linux认为：

- 对于第一种情况，算作block，因为Linux无法知道网络上对方是否会发数据。如果没数据发过来，对于调用`read`的程序来说，就只能“等”。
- 对于第二种情况，**不算做阻塞**。

>一个解释是，所谓阻塞是指操作系统可以预见这个阻塞会发生才会主动阻塞。例如当读取TCP连接的数据时，如果发现socket buffer里没有数据就可以确定定对方还没有发过来，于是阻塞；而对于普通磁盘文件的读写，也许磁盘运作期间会抖动，会短暂暂停，但是操作系统无法预见这种情况，只能视作不会阻塞，照样执行。

基于这个基本的设定，在讨论IO时，一定要严格区分网络IO和磁盘文件IO。NIO和IO多路复用只对网络IO有意义。

#### BIO

BIO是Blocking IO的意思。在类似于网络中进行`read`，`write`，`connect`一类的系统调用时会被卡住。

举个例子，当用`read`去读取网络的数据时，是无法预知对方是否已经发送数据的。因此在收到数据之前，能做的只有等待，直到对方把数据发过来，或者等到网络超时。等待期间不可处理第二个客户端建立的连接。

对于单线程的网络服务，这样做就会有卡死的问题。因为当等待时，整个线程会被挂起，无法执行，也无法做其他的工作。

> 顺便说一句，这种Block是不会影响同时运行的其他程序（进程）的，因为现代操作系统都是多任务的，任务之间的切换是抢占式的。这里Block只是指Block当前的进程。

于是，网络服务为了同时响应多个并发的网络请求，必须实现为多线程的。每个线程处理一个网络请求。线程数随着并发连接数线性增长。这的确能奏效。但这带来两个问题：

- 线程越多，Context Switch就越多，而Context Switch是一个比较重的操作，会无谓浪费大量的CPU。上下文切换（一个进程/线程切换到另一个进程/线程）。
- 每个线程会占用一定的内存作为线程的栈。比如有1000个线程同时运行，每个占用1MB内存，就占用了1个G的内存。

问题的关键在于，当调用`read`接受网络请求时，有数据到了就用，没数据到时，实际上是可以干别的。使用大量线程，仅仅是因为Block发生，没有其他办法。

当然你可能会说，是不是可以弄个线程池呢？这样既能并发的处理请求，又不会产生大量线程。但这样会限制最大并发的连接数。比如你弄4个线程，那么最大4个线程都Block了就没法响应更多请求了。

要是操作IO接口时，操作系统能够总是直接告诉有没有数据，而不是Block去等就好了。于是，NIO登场。

#### NIO

NIO是指将IO模式设为“Non-Blocking”模式，不让用户阻塞。使用单线程解决高并发的问题。

###### 区别

在BIO模式下，调用read，如果发现没数据已经到达，就会阻塞住。

在NIO模式下，调用read，如果发现没数据已经到达，就会立刻返回-1，并且errno被设为`EAGAIN`，提示你的应用程序现在没有数据可读请稍后再试。

不让程序阻塞

1. 等待连接和读数据不阻塞
2. 有人来连，设置为非阻塞读，将当前socket放入到list，遍历list，查看是否有人发数据
3. 没人来连，将当前socket放入到list，遍历list，查看是否有人发数据

每次循环的时候，都会遍历list列表，如果列表很大，若只有20%有读写，性能还是较低

使用轮询，不断的尝试有没有数据到达，有了就处理，没有（得到`EWOULDBLOCK`或者`EAGAIN`）就等一小会再试。这比之前BIO好多了，起码程序不会被卡死了。

但这样会带来两个新问题：

- 如果有大量文件描述符都要等，那么就得一个一个的read。这会带来大量的Context Switch（`read`是系统调用，每调用一次就得在用户态和核心态切换一次）。
- 休息一会的时间不好把握。这里是要猜多久之后数据才能到。等待时间设的太长，程序响应延迟就过大；设的太短，就会造成过于频繁的重试，干耗CPU而已。

要是操作系统能一口气告诉程序，哪些数据到了就好了。

#### IO多路复用

IO多路复用（IO Multiplexing) 是这么一种机制：程序注册一组socket文件描述符给操作系统，表示“我要监视这些fd是否有IO事件发生，有了就告诉程序处理”。

IO多路复用是要和NIO一起使用的。尽管在操作系统级别，NIO和IO多路复用是两个相对独立的事情。**NIO仅仅是指IO API总是能立刻返回，不会被Blocking**；而**IO多路复用仅仅是操作系统提供的一种便利的通知机制**。操作系统并不会强制这俩必须得一起用——你可以用NIO，但不用IO多路复用，就像上一节中的代码；也可以只用IO多路复用 + BIO，这时效果还是当前线程被卡住。但是，IO多路复用和NIO是要配合一起使用才有实际意义。因此，在使用IO多路复用之前，请总是先把fd设为`O_NONBLOCK`。

将list交给操作系统来处理，具体的方法有三个：select、poll和epoll。

#### 怎么判断写文件时Buffer已经写满？

1. 使用copy_from_user从用户空间拷贝fd_set到内核空间

2. 注册回调函数__pollwait

3. 遍历所有fd，调用其对应的poll方法（对于socket，这个poll方法是sock_poll，sock_poll根据情况会调用到tcp_poll,udp_poll或者datagram_poll）
4. 以tcp_poll为例，其核心实现就是__pollwait，也就是上面注册的回调函数。
5. __pollwait的主要工作就是把current（当前进程）挂到设备的等待队列中，不同的设备有不同的等待队列，对于tcp_poll来说，其等待队列是sk->sk_sleep（注意把进程挂到等待队列中并不代表进程已经睡眠了）。在设备收到一条消息（网络设备）或填写完文件数据（磁盘设备）后，会唤醒设备等待队列上睡眠的进程，这时current便被唤醒了。

6. poll方法返回时会返回一个描述读写操作是否就绪的mask掩码，根据这个mask掩码给fd_set赋值。

7. 如果遍历完所有的fd，还没有返回一个可读写的mask掩码，则会调用schedule_timeout是调用select的进程（也就是current）进入睡眠。当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程。如果超过一定的超时时间（schedule_timeout指定），还是没人唤醒，则调用select的进程会重新被唤醒获得CPU，进而重新遍历fd，判断有没有就绪的fd。

8. 把fd_set从内核空间拷贝到用户空间

#### 简述Linux的IO模型

见39