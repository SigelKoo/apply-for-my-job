# 生成分布式ID的方法

- 数据库自增ID
- 数据库水平拆分，设置初始值和相同的自增步长
- 批量申请自增ID
- UUID生成
- Redis的方式
- 雪花算法

## 数据库自增ID

在创建表的时候，指定主键auto_increment（自增）便可以实现

## 数据库水平拆分，设置初始值和相同的自增步长

「数据库水平拆分，设置初始值和相同的自增步长」是指在DB集群的环境下，将数据库进行水平划分，然后每个数据库设置「不同的初始值」和「相同的步长」，这样就能避免ID重复的情况

![1.png](http://dockone.io/uploads/article/20200720/415a500527a3c64743dca3d628f89961.png)

```
set @@auto_increment_offset = 1;     // 设置初始值
set @@auto_increment_increment = 2;  // 设置步长
```

扩容的情况是这种方法的一个缺点，上面我说的步长一般设置为数据库的数量，这是在确保后期不会扩容的情况下，若是确定后期会有扩容情况，在前期设计的的时候可以将步长设置长一点，「预留一些初始值给后续扩容使用」。

总之，这种方案还是优缺点的，缺点就是：「后期可能会面对无ID初始值可分的窘境，数据库总归是数据库，抗高并发也是有限的」。

## 批量申请自增ID

「批量申请自增ID」的解决方案可以解决无ID可分的问题，它的原理就是一次性给对应的数据库上分配一批的ID值进行消费，使用完了，再回来申请。

![2.png](http://dockone.io/uploads/article/20200720/bd156d3ade33cb0c6512b93ee634d075.png)

在设计的初始阶段可以设计一个有初始值字段，并有步长字段的表，当每次要申请批量ID的时候，就可以去该表中申请，每次申请后「初始值=上一次的初始值+步长」。

这样就能保持初始值是每一个申请的ID的最大值，避免了ID的重复，并且每次都会有ID使用，一次就会生成一批的id来使用，这样访问数据库的次数大大减少。

## UUID生成

UUID的核心思想是使用「机器的网卡、当地时间、一个随机数」来生成UUID。

使用UUID的方式只需要调用UUID.randomUUID().toString()就可以生成，这种方式方便简单，本地生成，不会消耗网络。

当时简单的东西，出现的问题就会越多，不利于存储，16字节128位，通常是以36位长度的字符串表示，很多的场景都不适合。

可以通过「当前的时间戳及机器mac地址」来生成，可以确保生成的UUID全球唯一。

## Redis的方式

Redis本身有incr和increby这样自增的命令，保证原子性，生成的ID也是有序的。

## 雪花算法

![3.png](http://dockone.io/uploads/article/20200720/2a152ac58344b739eb09671be7872e87.png)

第一位作为标识位，占用1bit，其值始终是0，没有实际作用。 

41bit是时间戳，毫秒级位单位，注意这里的时间戳并不是指当前时间的时间戳，而是值之间差（「当前时间-开始时间」）。开始时间一般是指ID生成器的开始时间，是由我们程序自己指定的。

后面的10bit：包括5位的「数据中心标识ID（datacenterId）和5位的机器标识ID（workerId）」，可以最多标识1024个节点（1<<10=1024）。

最后12位是序列号，12位的计数顺序支持每个节点每毫秒差生4096序列号（1<<12=4096）。

雪花算法使用数据中心ID和机器ID作为标识，不会产生ID的重复，并且是在本地生成，不会消耗网络，效率高，有数据显示，每秒能生成26万个ID。

雪花算法的计算依赖于时间，若是系统时间回拨，就会产生重复ID的情况。在雪花算法的实现中，若是其前置的时间等于当前的时间，就抛出异常，也可以关闭掉时间回拨。

# Redis如何实现分布式锁？

- 当在分布式模型下，数据只有一份（或有限制），此时需要利用锁的技术控制某一时刻修改数据的进程数。
- 与单机模式下的锁不仅需要保证进程可见，还需要考虑进程与锁之间的网络问题。（我觉得分布式情况下之所以问题变得复杂，主要就是需要考虑到**网络的延时和不可靠**。。。一个大坑）
- 分布式锁还是可以将标记存在内存，只是该内存不是某个进程分配的内存而是公共内存如 Redis、Memcache。至于利用数据库、文件等做锁与单机的实现是一样的，只要保证标记能互斥就行。

## 基于 redis 的 setnx()、expire() 方法做分布式锁

### setnx()

setnx 的含义就是 SET if Not Exists，其主要有两个参数 setnx(key, value)。该方法是原子的，如果 key 不存在，则设置当前 key 成功，返回 1；如果当前 key 已经存在，则设置当前 key 失败，返回 0。

### expire()

expire 设置过期时间，要注意的是 setnx 命令不能设置 key 的超时时间，只能通过 expire() 来对 key 设置。

### 使用步骤

1. setnx(lockkey, 1) 如果返回 0，则说明占位失败；如果返回 1，则说明占位成功

2. expire() 命令对 lockkey 设置超时时间，为的是避免死锁问题。

3. 执行完业务代码后，可以通过 delete 命令删除 key。

这个方案其实是可以解决日常工作中的需求的，但从技术方案的探讨上来说，可能还有一些可以完善的地方。**比如，如果在第一步 setnx 执行成功后，在 expire() 命令执行成功前，发生了宕机的现象，那么就依然会出现死锁的问题，所以如果要对其进行完善的话，可以使用 redis 的 setnx()、get() 和 getset() 方法来实现分布式锁。**

## 基于 redis 的 setnx()、get()、getset()方法做分布式锁

### getset()

这个命令主要有两个参数 getset(key，newValue)。该方法是原子的，对 key 设置 newValue 这个值，并且返回 key 原来的旧值。假设 key 原来是不存在的，那么多次执行这个命令，会出现下边的效果：

1. getset(key, "value1") 返回 null 此时 key 的值会被设置为 value1
2. getset(key, "value2") 返回 value1 此时 key 的值会被设置为 value2
3. 依次类推！

### 使用步骤

1. setnx(lockkey, 当前时间+过期超时时间)，如果返回 1，则获取锁成功；如果返回 0 则没有获取到锁，转向 2。
2. get(lockkey) 获取值 oldExpireTime ，并将这个 value 值与当前的系统时间进行比较，如果小于当前系统时间，则认为这个锁已经超时，可以允许别的请求重新获取，转向 3。
3. 计算 newExpireTime = 当前时间+过期超时时间，然后 getset(lockkey, newExpireTime) 会返回当前 lockkey 的值currentExpireTime。
4. 判断 currentExpireTime 与 oldExpireTime 是否相等，如果相等，说明当前 getset 设置成功，获取到了锁。如果不相等，说明这个锁又被别的请求获取走了，那么当前请求可以直接返回失败，或者继续重试。
5. 在获取到锁之后，当前线程可以开始自己的业务处理，当处理完毕后，比较自己的处理时间和对于锁设置的超时时间，如果小于锁设置的超时时间，则直接执行 delete 释放锁；如果大于锁设置的超时时间，则不需要再对锁进行处理。

## 多节点redis实现的分布式锁算法(RedLock):有效防止单点故障

假设有5个完全独立的redis主服务器

1. 获取当前时间戳

2. client尝试按照顺序使用相同的key,value获取所有redis服务的锁，在获取锁的过程中的获取时间比锁过期时间短很多，这是为了不要过长时间等待已经关闭的redis服务。并且试着获取下一个redis实例。

   比如：TTL为5s,设置获取锁最多用1s，所以如果一秒内无法获取锁，就放弃获取这个锁，从而尝试获取下个锁

3. client通过获取所有能获取的锁后的时间减去第一步的时间，这个时间差要小于TTL时间并且至少有3个redis实例成功获取锁，才算真正的获取锁成功

4. 如果成功获取锁，则锁的真正有效时间是 TTL减去第三步的时间差 的时间；比如：TTL 是5s,获取所有锁用了2s,则真正锁有效时间为3s(其实应该再减去时钟漂移);

5. 如果客户端由于某些原因获取锁失败，便会开始解锁所有redis实例；因为可能已经获取了小于3个锁，必须释放，否则影响其他client获取锁

算法示意图如下：

​               ![img](https://img2018.cnblogs.com/blog/1176050/201904/1176050-20190409171037788-1969443095.png)

 

 

**RedLock算法是否是异步算法？？**

可以看成是同步算法；因为 即使进程间（多个电脑间）没有同步时钟，但是每个进程时间流速大致相同；并且时钟漂移相对于TTL叫小，可以忽略，所以可以看成同步算法；（不够严谨，算法上要算上时钟漂移，因为如果两个电脑在地球两端，则时钟漂移非常大）

**RedLock失败重试**

当client不能获取锁时，应该在随机时间后重试获取锁；并且最好在同一时刻并发的把set命令发送给所有redis实例；而且对于已经获取锁的client在完成任务后要及时释放锁，这是为了节省时间；

**RedLock释放锁**

由于释放锁时会判断这个锁的value是不是自己设置的，如果是才删除；所以在释放锁时非常简单，只要向所有实例都发出释放锁的命令，不用考虑能否成功释放锁；

**RedLock注意点（Safety arguments）:**

1. 先假设client获取所有实例，所有实例包含相同的key和过期时间(TTL) ,但每个实例set命令时间不同导致不能同时过期，第一个set命令之前是T1,最后一个set命令后为T2,则此client有效获取锁的最小时间为TTL-(T2-T1)-时钟漂移;

2. 对于以N/2+ 1(也就是一半以 上)的方式判断获取锁成功，是因为如果小于一半判断为成功的话，有可能出现多个client都成功获取锁的情况， 从而使锁失效

3. 一个client锁定大多数事例耗费的时间大于或接近锁的过期时间，就认为锁无效，并且解锁这个redis实例(不执行业务) ;只要在TTL时间内成功获取一半以上的锁便是有效锁;否则无效

**系统有活性的三个特征**

1.能够自动释放锁

2.在获取锁失败（不到一半以上），或任务完成后 能够自动释放锁，不用等到其自动过期

3.在client重试获取哦锁前（第一次失败到第二次重试时间间隔）大于第一次获取锁消耗的时间；

4.重试获取锁要有一定次数限制

**RedLock性能及崩溃恢复的相关解决方法**

1.如果redis没有持久化功能，在clientA获取锁成功后，所有redis重启，clientB能够再次获取到锁，这样违法了锁的排他互斥性;

2.如果启动AOF永久化存储，事情会好些， 举例:当我们重启redis后，由于redis过期机制是按照unix时间戳走的，所以在重启后，然后会按照规定的时间过期，不影响业务;但是由于AOF同步到磁盘的方式默认是每秒-次，如果在一秒内断电，会导致数据丢失，立即重启会造成锁互斥性失效;但如果同步磁盘方式使用Always(每一个写命令都同步到硬盘)造成性能急剧下降;所以在锁完全有效性和性能方面要有所取舍; 

3.有效解决既保证锁完全有效性及性能高效及即使断电情况的方法是redis同步到磁盘方式保持默认的每秒，在redis无论因为什么原因停掉后要等待TTL时间后再重启(学名:**延迟重启**) ;缺点是 在TTL时间内服务相当于暂停状态;

总结：

1.TTL时长 要大于正常业务执行的时间+获取所有redis服务消耗时间+时钟漂移

2.获取redis所有服务消耗时间要 远小于TTL时间，并且获取成功的锁个数要 在总数的一般以上:N/2+1

3.尝试获取每个redis实例锁时的时间要 远小于TTL时间

4.尝试获取所有锁失败后 重新尝试一定要有一定次数限制

5.在redis崩溃后（无论一个还是所有），要延迟TTL时间重启redis

6.在实现多redis节点时要结合单节点分布式锁算法 共同实现
